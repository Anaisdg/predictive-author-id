{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Predictions\n",
    "\n",
    "This file runs the same code used to investigate the training data on the predicted author results for the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in some helpful libraries\n",
    "import nltk # the natural langauage toolkit, open-source NLP\n",
    "import pandas as pd # dataframes\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import string\n",
    "\n",
    "# initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in training data\n",
    "This data will be used to develop an identification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id21695</td>\n",
       "      <td>The families of Berlifitzing and Metzengerstei...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id11413</td>\n",
       "      <td>I still quickened my pace.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11977</td>\n",
       "      <td>Mr. Maelzel, to be sure, is not very tall, but...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id11677</td>\n",
       "      <td>here Legrand touched each of Jupiter's eyes.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id24232</td>\n",
       "      <td>The next morning he was quite recovered, so fa...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id21695  The families of Berlifitzing and Metzengerstei...    EAP\n",
       "1  id11413                         I still quickened my pace.    EAP\n",
       "2  id11977  Mr. Maelzel, to be sure, is not very tall, but...    EAP\n",
       "3  id11677       here Legrand touched each of Jupiter's eyes.    EAP\n",
       "4  id24232  The next morning he was quite recovered, so fa...    EAP"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Read our train data into a dataframe\n",
    "texts = pd.read_csv(\"Data_Output/equal_length_test.csv\", encoding = 'latin-1')\n",
    "\n",
    "# look at the first few rows of texts\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial data organization\n",
    "* Group data by author\n",
    "* Create object with all sentences for each author\n",
    "* Create object with all token for each author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data by author\n",
    "byAuthor = texts.groupby(\"author\")\n",
    "\n",
    "# create empty dictionary to store combined sentences for each author\n",
    "sentence_dict = {}\n",
    "\n",
    "# create empty dictionary to store tokenized sentences for each author\n",
    "token_dict = {}\n",
    "\n",
    "# for each author...\n",
    "for name, group in byAuthor:\n",
    "    # get all of the sentences they wrote and collapse them into a\n",
    "    # single long string\n",
    "    sentences = group['text'].str.cat(sep = ' ')\n",
    "    \n",
    "    # convert everything to lower case (so \"The\" and \"the\" get counted as \n",
    "    # the same word rather than two different words)\n",
    "    sentences = sentences.lower()\n",
    "    \n",
    "    sentence_dict[name] = (sentences)\n",
    "    \n",
    "    # split the text into individual tokens (words)  \n",
    "    tokens = nltk.tokenize.word_tokenize(sentences)\n",
    "    \n",
    "    token_dict[name] = (tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic word & sentence structure analysis\n",
    "* Average sentence length for each author\n",
    "* Average word length for each author\n",
    "* Total word variety for each author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Avg sentence length by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of texts to work with\n",
    "texts_len = texts.copy()\n",
    "\n",
    "# add the sentence length for each row\n",
    "texts_len['sentence_len'] = [len(str.split(s)) for s in texts['text']]\n",
    "\n",
    "# create column to populate with the unique words used in each sentence\n",
    "texts_len['vocab'] = [len(set(str.split(s))) for s in texts_len['text']]\n",
    "\n",
    "texts_len.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get summary data for each author on the sentence length\n",
    "sentence_summary = pd.DataFrame(texts_len.groupby('author')['sentence_len'].describe())\n",
    "sentence_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.bar(np.arange(3), sentence_summary['mean'], yerr=sentence_summary['std'], color = ['red', 'blue', 'purple'], alpha = 0.6)\n",
    "plt.xticks(np.arange(3), sentence_summary.index)\n",
    "plt.title(\"Average Sentence Length by Author\")\n",
    "plt.xlabel(\"Author\")\n",
    "plt.ylabel(\"Sentence Length\")\n",
    "plt.savefig('test_plots/avg_sentence_length.png', dpi = 300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_len.boxplot(column = 'sentence_len', by = 'author')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Avg word length by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dictionary to populate with average word length by author \n",
    "wordlengthByAuthor = {}\n",
    "\n",
    "# for each author...\n",
    "for key, value in token_dict.items():\n",
    "    \n",
    "    # characterizing words by different lengths\n",
    "    small_words = len([w for w in value if len(w) <= 5])\n",
    "    med_words = len([w for w in value if len(w) > 5 & len(w) <10])\n",
    "    large_words = len([w for w in value if len(w) > 9 & len(w) <15])\n",
    "    xlarge_words = len([w for w in value if len(w) > 14 & len(w) <15])\n",
    "    # Average Word Length for each author\n",
    "    avg_word_length = np.mean([len(w) for w in value])\n",
    "    # Std Dev of word length for each author\n",
    "    std_word_length = np.std([len(w) for w in value])\n",
    "    \n",
    "    wordlengthByAuthor[key] = ([small_words, med_words, large_words, xlarge_words,avg_word_length, std_word_length])\n",
    "    \n",
    "wordlength_df = pd.DataFrame(wordlengthByAuthor, index = ['small (<5)', 'med (5-9)', 'large (10-14)', \n",
    "                                                          'xlarge (14+)', 'avg length', 'std'])   \n",
    "wordlength_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing the 'avg length' row from the avg_len_df\n",
    "avg_len_df = wordlength_df.iloc[4,:]\n",
    "avg_len_df = pd.DataFrame(avg_len_df)\n",
    "\n",
    "# slicing the 'std' row from the std_wordlength_df\n",
    "std_df = wordlength_df.iloc[5,:]\n",
    "std_df = pd.DataFrame(std_df)\n",
    "\n",
    "# creating the plots\n",
    "y_axis = avg_len_df['avg length']\n",
    "x_axis = np.arange(0,len(y_axis),1)\n",
    "\n",
    "# setting tick positions and labels\n",
    "plt.xticks(x_axis,[\"EAP\",\"HPL\",\"MWS\"])\n",
    "colors=['red','blue','purple']\n",
    "\n",
    "plot = plt.bar(x_axis,y_axis,yerr=std_df['std'],color=colors,align='center',alpha=0.5)\n",
    "plt.title(\"Avg Word Length / Author\")\n",
    "plt.ylabel(\"avg word length\")\n",
    "\n",
    "plt.savefig('test_plots/avg_word_length.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the words by category for each author with std error bars\n",
    "wordlength_df2 = wordlength_df.iloc[0:4,:]\n",
    "wordlength_df2.plot(kind='bar',title = 'Total Words / Author',\n",
    "                    legend=True, rot=0, figsize=(10, 5))\n",
    "plt.ylabel('Total no of Words')\n",
    "plt.savefig('test_plots/total_words_by_size.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get total word variety by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dictionary to store the number of unique words used by each author\n",
    "vocabulary = {}\n",
    "\n",
    "# for each author\n",
    "for key, value in token_dict.items():\n",
    "    unique_words = set(value)\n",
    "    len_unique_words = len(unique_words)\n",
    "    vocabulary[key] = (len_unique_words)\n",
    "    \n",
    "#vocabulary_df = pd.DataFrame(vocabulary)\n",
    "#vocabulary_df\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.bar(np.arange(3), vocabulary.values(), color = ['red', 'blue', 'purple'], alpha = 0.5)\n",
    "plt.xticks(np.arange(3), vocabulary.keys())\n",
    "plt.title(\"Vocabulary Size by Author\")\n",
    "plt.xlabel(\"Author\")\n",
    "plt.ylabel(\"Vocabulary Size\")\n",
    "plt.savefig('test_plots/vocabulary_size.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronoun usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get parts-of-speech for each author using \n",
    "\n",
    "firstPerson = ['i', 'me', 'we', 'us']\n",
    "secondPerson = ['you'] \n",
    "thirdPerson = ['he', 'she', 'it', 'him', 'her', 'it', 'they', 'them']\n",
    "firstCount = {}\n",
    "secondCount = {}\n",
    "thirdCount = {}\n",
    "\n",
    "for key in token_dict:\n",
    "    counts = Counter(token_dict[key])\n",
    "    firstCount[key] = { pronoun: counts[pronoun] for pronoun in firstPerson }\n",
    "    secondCount[key] = { pronoun: counts[pronoun] for pronoun in secondPerson } \n",
    "    thirdCount[key] = { pronoun: counts[pronoun] for pronoun in thirdPerson } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack dictionaries into DataFrames \n",
    "authors = ['EAP', 'HPL', 'MWS']\n",
    "first = []\n",
    "second = []\n",
    "third = []\n",
    "\n",
    "# df for first person pronouns\n",
    "for name in authors:\n",
    "    temp = []\n",
    "    pronoun = []\n",
    "    values = []\n",
    "    for key, value in firstCount[name].items():\n",
    "        temp = [key,value]\n",
    "        pronoun.append(temp[0])\n",
    "        values.append(temp[1])\n",
    "    df1 = pd.DataFrame({'Pronoun': pronoun,\n",
    "                 name: values,\n",
    "                }).set_index('Pronoun')\n",
    "    first.append(df1)\n",
    "first = pd.concat(first, axis=1)\n",
    "\n",
    "# df for second person pronouns\n",
    "for name in authors:\n",
    "    temp = []\n",
    "    pronoun = []\n",
    "    values = []\n",
    "    for key, value in secondCount[name].items():\n",
    "        temp = [key,value]\n",
    "        pronoun.append(temp[0])\n",
    "        values.append(temp[1])\n",
    "    df1 = pd.DataFrame({'Pronoun': pronoun,\n",
    "                 name: values,\n",
    "                }).set_index('Pronoun')\n",
    "    second.append(df1)\n",
    "second = pd.concat(second, axis=1)\n",
    "\n",
    "# df for third person pronouns\n",
    "for name in authors:\n",
    "    temp = []\n",
    "    pronoun = []\n",
    "    values = []\n",
    "    for key, value in thirdCount[name].items():\n",
    "        temp = [key,value]\n",
    "        pronoun.append(temp[0])\n",
    "        values.append(temp[1])\n",
    "    df1 = pd.DataFrame({'Pronoun': pronoun,\n",
    "                 name: values,\n",
    "                }).set_index('Pronoun')\n",
    "    third.append(df1)\n",
    "third = pd.concat(third, axis=1)\n",
    "\n",
    "third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pronoun DataFrames\n",
    "first.plot(kind ='bar')\n",
    "plt.title('First Person Pronouns')\n",
    "plt.savefig('test_plots/first_person.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second.plot(kind ='bar')\n",
    "plt.title('Second Person Pronouns')\n",
    "plt.savefig('test_plots/second_person.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third.plot(kind = 'bar')\n",
    "plt.title('Third Person Pronouns')\n",
    "plt.savefig('test_plots/third_person.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parts-of-speech for each author using \n",
    "partOfSpeech = {}\n",
    "\n",
    "for key, value in token_dict.items():\n",
    "    \n",
    "    partOfSpeech[key] = nltk.pos_tag(value)\n",
    "\n",
    "partOfSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the the number of parts of speech for each author \n",
    "\n",
    "# create an empty dictonary to hold all of the counts\n",
    "partOfSpeechCounts = {}\n",
    "for name in partOfSpeech: \n",
    "     partOfSpeechCounts[name] = Counter(elem[1] for elem in partOfSpeech[name])\n",
    "        \n",
    "partOfSpeechCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the above in a dataframe\n",
    "speech_parts_df = pd.DataFrame(partOfSpeechCounts)\n",
    "speech_parts_df\n",
    "speech_parts_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the string library, pull the list of punctuations\n",
    "punctuation = set(string.punctuation)\n",
    "# Add in the missing punctuations \n",
    "punctuation.add('``')\n",
    "punctuation.add('\\'\\'')\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the rows of speech_parts_df and drop the rows if the index is not punctuation mark\n",
    "for index,row in speech_parts_df.iterrows():\n",
    "    if(index not in punctuation):\n",
    "        speech_parts_df = speech_parts_df.drop(index)\n",
    "        \n",
    "speech_parts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot for count of punctuation types by author\n",
    "xticks = [\"''\",',','.',':','``']\n",
    "x_axis = np.arange(0,len(speech_parts_df),1)\n",
    "\n",
    "y_EAP_axis = speech_parts_df['EAP']\n",
    "y_HPL_axis = speech_parts_df[\"HPL\"]\n",
    "y_MWS_axis = speech_parts_df[\"MWS\"]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "EAP_handle = plt.scatter(x=x_axis,y=y_EAP_axis,marker='o',color='r',edgecolors='black',s=200,alpha=0.7,label=\"EAP\") \n",
    "HPL_handle = plt.scatter(x=x_axis,y=y_HPL_axis,marker='o',color='b',edgecolors='black',s=200,alpha=0.7,label=\"HPL\") \n",
    "MWS_handle = plt.scatter(x=x_axis,y=y_MWS_axis,marker='o',color='g',edgecolors='black',s=200,alpha=0.7,label=\"MWS\") \n",
    "\n",
    "plt.legend(handles=[EAP_handle,HPL_handle,MWS_handle],loc='best')\n",
    "\n",
    "plt.xticks(x_axis,xticks,fontsize=30)\n",
    "plt.ylabel(\"Counter\")\n",
    "plt.title(\"Punctuation Count By Author\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speech_parts_df.plot(kind = 'bar')\n",
    "xticks = [\"''\",',','.',':','``']\n",
    "plt.xticks(x_axis,xticks,fontsize=30)\n",
    "plt.ylabel(\"Counter\")\n",
    "plt.title(\"Punctuation Count By Author\")\n",
    "plt.savefig('test_plots/punctuation.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word frequencies by author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiments analysis including : avg sentiment scores, std of sentiment scores, scatter plots for compound sentiment \n",
    "# analysis by Author \n",
    "\n",
    "sentimentByAuthor = {}\n",
    "\n",
    "for name, group in byAuthor:\n",
    "# collect sentiment analysis scores\n",
    "    results = [analyzer.polarity_scores(w) for w in group['text']]\n",
    "    sentimentByAuthor[name] = (results)\n",
    "    \n",
    "sentimentByAuthor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dictionaries to contain avg sentiment values for all sentences by authors\n",
    "avg_compound = {}\n",
    "avg_pos = {}\n",
    "avg_neu = {}\n",
    "avg_neg = {}\n",
    "\n",
    "# create empty dictionaries to contain std of sentiments for all sentences by authors\n",
    "std_compound = {}\n",
    "std_pos = {}\n",
    "std_neu = {}\n",
    "std_neg = {}\n",
    "\n",
    "for name, group in byAuthor:\n",
    "    compound = [d['compound'] for d in sentimentByAuthor[name]]\n",
    "    pos = [d['pos'] for d in sentimentByAuthor[name]]\n",
    "    neu = [d['neu'] for d in sentimentByAuthor[name]]\n",
    "    neg = [d['neg'] for d in sentimentByAuthor[name]]\n",
    "    \n",
    "    avg_compound[name] = sum(compound) / len(sentimentByAuthor[name])\n",
    "    avg_pos[name] = sum(pos) / len(sentimentByAuthor[name])\n",
    "    avg_neu[name] = sum(neu) / len(sentimentByAuthor[name])\n",
    "    avg_neg[name] = sum(neg) / len(sentimentByAuthor[name])\n",
    "    \n",
    "    std_compound[name] = np.std(compound, axis = None)\n",
    "    std_pos[name] = np.std(pos, axis = None)\n",
    "    std_neu[name] = np.std(neu, axis = None)\n",
    "    std_neg[name] = np.std(neg, axis = None)\n",
    "    \n",
    "    # plot compound sentiment analysis for each author\n",
    "    #plt.figure(figsize = (10,10))\n",
    "    plt.title('Compound Sentiment Polarity for ' + name)\n",
    "    plt.xlabel('Sentences')\n",
    "    plt.ylabel('Sentence Polarity')\n",
    "    plt.plot(np.arange(0, len(compound)), compound,  marker=\"o\", linewidth=0.5,\n",
    "         alpha=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot bar graph of avg sentiment scores with error bars for each author \n",
    "\n",
    "\n",
    "avg_sentiments = {'EAP':[avg_compound['EAP'], avg_pos['EAP'], avg_neu['EAP'] ,avg_neg['EAP']],\n",
    "                  'HPL': [avg_compound['HPL'], avg_pos['HPL'], avg_neu['HPL'] ,avg_neg['HPL']],\n",
    "                  'MWS': [avg_compound['MWS'], avg_pos['MWS'], avg_neu['MWS'] ,avg_neg['MWS']]}\n",
    "std_sentiments = {'EAP':[std_compound['EAP'], std_pos['EAP'], std_neu['EAP'] ,std_neg['EAP']],\n",
    "                  'HPL': [std_compound['HPL'], std_pos['HPL'], std_neu['HPL'] ,std_neg['HPL']],\n",
    "                  'MWS': [std_compound['MWS'], std_pos['MWS'], std_neu['MWS'] ,std_neg['MWS']]}\n",
    "indexNames = ['Avg Compound', 'Avg Pos', 'Avg Neu', 'Avg Neg']\n",
    "avg_sentimentsdf = pd.DataFrame(avg_sentiments, index = indexNames) \n",
    "std_sentimentsdf = pd.DataFrame(std_sentiments, index = indexNames)\n",
    "    \n",
    "#plt.figure(figsize = (10,10))\n",
    "ax = avg_sentimentsdf.plot(kind = 'bar', title = 'Avg Sentiment Scores', \n",
    "                      legend=True , yerr= std_sentimentsdf, figsize=(10, 5))\n",
    "plt.savefig('test_plots/sentiment_scores.png', dpi = 300)\n",
    "plt.show()\n",
    "\n",
    "avg_sentimentsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grps = pd.unique(texts_len.author.values)\n",
    "sent_len = {grp:texts_len['sentence_len'][texts_len.author == grp] for grp in grps}\n",
    "\n",
    "F, p = stats.f_oneway(sent_len['EAP'], sent_len['HPL'], sent_len['MWS'])\n",
    "\n",
    "print(F)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {grp:texts_len['vocab'][texts_len.author == grp] for grp in grps}\n",
    "\n",
    "F, p = stats.f_oneway(vocab['EAP'], vocab['HPL'], vocab['MWS'])\n",
    "\n",
    "print(F)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dictionary to populate with word lengths by author \n",
    "wordlengthByAuthor_all = {}\n",
    "\n",
    "# for each author...\n",
    "for key, value in token_dict.items():\n",
    "    \n",
    "    # characterizing words by different lengths\n",
    "    word_len = [len(w) for w in value]\n",
    "    wordlengthByAuthor_all[key] = word_len\n",
    "    \n",
    "F, p = stats.f_oneway(wordlengthByAuthor_all['EAP'], wordlengthByAuthor_all['HPL'], wordlengthByAuthor_all['MWS'])\n",
    "\n",
    "print(F)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
